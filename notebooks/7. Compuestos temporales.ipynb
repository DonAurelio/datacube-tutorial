{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/DonAurelio/open-datacube-bac-training/main/docs/banner.png\" alt=\"Deparatemento de Ingeniería de Sistemas y Computación, Universidad de los Andes\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Aplicación de algoritmos para  el análisis de coberturas\n",
    "\n",
    "**Introducción**\n",
    "\n",
    "Los compuestos temporales reducen la incidencia de lasnubes en imágenes de satélite. Para ello se realiza un proceso de selección de pixeles en una serie de tiempo para una imagen. Como resultado del proceso de selección, se obtiene una imagen o composición que contiene pixeles de diferentes periodos de tiempo. Estos píxeles son seleccionados mediante la aplicación de un criterio de selección de pixeles. Algunos métodos de selección de píxeles conocidos son: Most Recent / Least Recent Pixel, Maximum Value Composite (MVC), Median Composite, Geomedian Composite, Best Pixel, entre otros. En el presente notebook exploraremos uno de los compuestos mas conocidos, el compuesto de medianas.\n",
    "\n",
    "**Contenido**\n",
    "\n",
    "1. Importar librerías\n",
    "2. Consulta del área de estudio\n",
    "3. Cálculo del compuesto de medianas\n",
    "4. Guardar resultados de análisis en fortamto netcdf\n",
    "5. Guardar resultados de análisis en formato geotiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar librerías\n",
    "\n",
    "En esta sección se importan las librerías cuya funicionalidades particulares son requeridas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# las funcionalidades del open data cube son accedidas \n",
    "# por medio de la librería datacube\n",
    "import datacube\n",
    "\n",
    "# Librería para cálculo en matrices\n",
    "import numpy as np\n",
    "\n",
    "# Manipulación de datasets\n",
    "import xarray as xr\n",
    "\n",
    "# Manipulación de datos raster\n",
    "import rasterio\n",
    "\n",
    "# Librería usada para la carga de polígonos\n",
    "import geopandas as gpd\n",
    "\n",
    "# Librería usada para visualización de datos\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Desactiva los warnings en el notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Configuración de Drivers para leer polígonos en formato KMLs\n",
    "gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Consulta del área de estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Opción 1) Consultar un área a partir de un polígono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del archivo .kml\n",
    "df_polygon = gpd.read_file(\"polygon/tota_lake.kml\",driver='KML')\n",
    "df_polygon = df_polygon.to_crs('EPSG:4326')\n",
    "\n",
    "# Pintar el polígono seleccionado\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "df_polygon.boundary.plot(ax=ax,color='red')\n",
    "\n",
    "# Obtención de la geometría del polígono del GeoDataFrame\n",
    "geometry_predio = df_polygon['geometry'][0]\n",
    "\n",
    "# Obtención de los límites del cuadrado que enmarca el polígono\n",
    "minx, miny, maxx, maxy = geometry_predio.bounds\n",
    "\n",
    "# Aumento del aŕea del cuadrado para \"EPSG:32719\"\n",
    "buffer = 0.001\n",
    "\n",
    "minx = minx - buffer\n",
    "miny = miny - buffer\n",
    "maxx = maxx + buffer\n",
    "maxy = maxy + buffer\n",
    "\n",
    "# Parámetros de área a ser consultada\n",
    "set_study_area_lat = (miny,maxy)\n",
    "set_study_area_lon = (minx,maxx)\n",
    "\n",
    "print(f'    latitude={set_study_area_lat},')\n",
    "print(f'    longitude={set_study_area_lon},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Opción 2) Consultar un área a partir de un punto\n",
    "\n",
    "Los coordenadas del punto a seleccionar pueden ser obtenidas a travez de herramientas GIS como google maps. Este punto debe estar comprendido en el área que desea estudiar. El punto definido será empleado para la generación de un cuadrado que finálmente será usado para consultar el área de estudio. La variable `buffer` permite ampliar o disminuir las dimensiones del cuadrado. Lo anterior es equivalente a disminuir o ampliar el área de estudio a consultar en el open data cube.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DonAurelio/open-datacube-bac-training/main/docs/latlong_buffer.png\" alt=\"Definición área de estudio\" width=\"20%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de las coordenadas del punto\n",
    "central_lat = 5.547964746532565\n",
    "central_lon =  -72.9284962779124\n",
    "\n",
    "# Aumento del aŕea del cuadrado para \"EPSG:4326\"\n",
    "# 11.1 kilómetros\n",
    "buffer = 0.1\n",
    "\n",
    "# Calculo del cuadro delimitador (bounding box) para el área de estudio\n",
    "set_study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "set_study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "print(f'    latitude={set_study_area_lat},')\n",
    "print(f'    longitude={set_study_area_lon},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta de información sobre el área de interes por medio del open data cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app=\"Cana\")\n",
    "\n",
    "dataset = dc.load(\n",
    "    product=\"s2_sen2cor_ard_granule_EO3\",\n",
    "    latitude=(5.447964746532565, 5.647964746532565),\n",
    "    longitude=(-73.0284962779124, -72.82849627791241),\n",
    "    time=('2021-01-01', '2021-02-01'),\n",
    "    measurements=[\"red\",\"blue\",\"green\",\"nir\",\"swir1\",\"swir2\",\"scl\"],\n",
    "    crs=\"EPSG:4326\",\n",
    "    output_crs=\"EPSG:4326\",\n",
    "    resolution=(-0.00008983111,0.00008971023)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En caso de que la consulta arroje como resultado varios periodos de tiempo, el código que se muestra a continuación permite visualizar la imágen en RGB de todos estos periodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = dataset[[\"red\",\"green\",\"blue\"]].to_array(dim='color')\n",
    "rgb = rgb.transpose(*(rgb.dims[1:]+rgb.dims[:1]))  # make 'color' the last dimension\n",
    "img = rgb.plot.imshow(col='time',col_wrap=4,add_colorbar=False,vmin=0,vmax=1500)\n",
    "\n",
    "for axes in img.axes.flat:\n",
    "    df_polygon.boundary.plot(ax=axes,markersize=20,color='blue',marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cálculo del compuesto de medianas\n",
    "\n",
    "El compuesto de medianas (Median Composite), se basa en el cálculo de la mediana  de  cada  píxel  en  una  serie  de  tiempo. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/DonAurelio/open-datacube-bac-training/main/docs/composite.png\" alt=\"Compuesto de medianas\" width=\"60%\">\n",
    "\n",
    "El cálculo del compuesto de medianas comprende dos pasos: \n",
    "\n",
    "1. **Aplicación de la máscara de nubes**\n",
    "2. **Generación del compuesto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones auxiliares para el cálculo del compuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_bits(land_cover_endcoding, data_array, cover_type):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Unpack bits for end of ls7 and ls8 functions \n",
    "    -----\n",
    "    Input:\n",
    "        land_cover_encoding(dict hash table) land cover endcoding provided by ls7 or ls8\n",
    "        data_array( xarray DataArray)\n",
    "        cover_type(String) type of cover\n",
    "    Output:\n",
    "        unpacked DataArray\n",
    "    \"\"\"\n",
    "    data = data_array.data\n",
    "    if isinstance(data, np.ndarray):\n",
    "        boolean_mask = np.isin(data, land_cover_endcoding[cover_type]) \n",
    "    elif isinstance(data, dask.array.core.Array):\n",
    "        boolean_mask = dask.array.isin(data, land_cover_endcoding[cover_type])\n",
    "    return xr.DataArray(boolean_mask.astype(bool),\n",
    "                        coords = data_array.coords,\n",
    "                        dims = data_array.dims,\n",
    "                        name = cover_type + \"_mask\",\n",
    "                        attrs = data_array.attrs)\n",
    "\n",
    "def s2_unpack_qa(data_array , cover_type):\n",
    "\n",
    "    land_cover_endcoding = dict( \n",
    "        no_data                 =[0] ,\n",
    "        saturated_or_defective  =[1],\n",
    "        dark_area               =[2],\n",
    "        cloud_shadow            =[3],\n",
    "        vegetation              =[4],\n",
    "        not_vegetated           =[5],\n",
    "        water                   =[6],\n",
    "        unclassified            =[7],\n",
    "        cloud_medium_prob       =[8],\n",
    "        cloud_high_prob         =[9],\n",
    "        thin_cirrus             =[10],\n",
    "        snow                    =[11]\n",
    "    )\n",
    "    return unpack_bits(land_cover_endcoding, data_array, cover_type)\n",
    "\n",
    "def qa_clean_mask(dataset, platform, cover_types=['vegetation', 'not_vegetated']):\n",
    "    \"\"\"\n",
    "    Returns a clean_mask for `dataset` that masks out various types of terrain cover using the\n",
    "    Landsat pixel_qa band. Note that Landsat masks specify what to keep, not what to remove.\n",
    "    This means that using `cover_types=['clear', 'water']` should keep only clear land and water.\n",
    "\n",
    "    See \"pixel_qa band\" here: https://landsat.usgs.gov/landsat-surface-reflectance-quality-assessment\n",
    "    and Section 7 here: https://landsat.usgs.gov/sites/default/files/documents/lasrc_product_guide.pdf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset: xarray.Dataset\n",
    "        An xarray (usually produced by `datacube.load()`) that contains a `pixel_qa` data\n",
    "        variable.\n",
    "    platform: str\n",
    "        A string denoting the platform to be used. Can be \"LANDSAT_5\", \"LANDSAT_7\", or\n",
    "        \"LANDSAT_8\".\n",
    "    cover_types: list\n",
    "        A list of the cover types to include. Adding a cover type allows it to remain in the masked data.\n",
    "        Cover types for all Landsat platforms include:\n",
    "        ['fill', 'clear', 'water', 'shadow', 'snow', 'cloud', 'low_conf_cl', 'med_conf_cl', 'high_conf_cl'].\n",
    "\n",
    "        'fill' removes \"no_data\" values, which indicates an absense of data. This value is -9999 for Landsat platforms.\n",
    "        Generally, don't use 'fill'.\n",
    "        'clear' allows only clear terrain. 'water' allows only water. 'shadow' allows only cloud shadows.\n",
    "        'snow' allows only snow. 'cloud' allows only clouds, but note that it often only selects cloud boundaries.\n",
    "        'low_conf_cl', 'med_conf_cl', and 'high_conf_cl' denote low, medium, and high confidence in cloud coverage.\n",
    "        'low_conf_cl' is useful on its own for only removing clouds, however, 'clear' is usually better suited for this.\n",
    "        'med_conf_cl' is useful in combination with 'low_conf_cl' to allow slightly heavier cloud coverage.\n",
    "        Note that 'med_conf_cl' and 'cloud' are very similar.\n",
    "        'high_conf_cl' is useful in combination with both 'low_conf_cl' and 'med_conf_cl'.\n",
    "\n",
    "        For Landsat 8, there are more cover types: ['low_conf_cir', 'high_conf_cir', 'terrain_occ'].\n",
    "        'low_conf_cir' and 'high_conf_cir' denote low and high confidence in cirrus clouds.\n",
    "        'terrain_occ' allows only occluded terrain.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clean_mask: xarray.DataArray\n",
    "        An xarray DataArray with the same number and order of coordinates as in `dataset`.\n",
    "    \"\"\"\n",
    "    processing_options = {\n",
    "        \"SENTINEL_2\": s2_unpack_qa\n",
    "    }\n",
    "\n",
    "    clean_mask = None\n",
    "    # Keep all specified cover types (e.g. 'clear', 'water'), so logically or the separate masks.\n",
    "    for i, cover_type in enumerate(cover_types):\n",
    "        cover_type_clean_mask = processing_options[platform](dataset.scl, cover_type)\n",
    "        clean_mask = cover_type_clean_mask if i == 0 else (clean_mask | cover_type_clean_mask)\n",
    "    return clean_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación de la máscara de nubes\n",
    "\n",
    "Durante el proceso de corrección de las imágenes satelitales Sentinel 2 se genera una banda de calidad llamada `scl`. Esta banda de calidad es el resultado de la ejecución de varíos algoirtmos especializados de detección de coberturas específicas. Más información sobre estos algoritmos se muestra [aquí](https://sentinel.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm). \n",
    "\n",
    "La tabla mostrada a continuación, presenta los valores de la banda de calidad `scl`. Es en base a esta banda que se aplica el algoirtmo de enmascaramiento de nubes (remover las nubes). \n",
    "\n",
    "| Atributo                 | Valor del Píxel |\n",
    "|--------------------------|-----------------|\n",
    "| No Data                  | 0               |\n",
    "| Saturated or defective   | 1               |\n",
    "| Dark area Pixels         | 2               |\n",
    "| Cloud Shadows            | 3               |\n",
    "| Vegetation               | 4               |\n",
    "| Not vegetated            | 5               |\n",
    "| Water                    | 6               |\n",
    "| Unclassified             | 7               |\n",
    "| Cloud Medium probability | 8               |\n",
    "| Cloud High probability   | 9               |\n",
    "| Thin Cirrus              | 10              |\n",
    "| Snow                     | 11              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.colors.ListedColormap(\n",
    "    [\n",
    "        '#000000', \n",
    "        '#fe0000',\n",
    "        '#404040', \n",
    "        '#833d0c', \n",
    "        '#00ff01', \n",
    "        '#ffff01', \n",
    "        '#0000cc', \n",
    "        '#757170', \n",
    "        '#aeaaa9', \n",
    "        '#d0ced0', \n",
    "        '#00ccff', \n",
    "        '#ff66ff'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Rango de valores establecidos para cada color\n",
    "bounds = [0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "\n",
    "# Genera una capa de normalización de los datos basada en los intérvalos establecidos en 'bounds'\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Generación de la imágen\n",
    "img = dataset.scl.plot(col='time',col_wrap=4, cmap=cmap, norm=norm)\n",
    "\n",
    "classification_labels = [\n",
    "    'no_data',\n",
    "    'saturated_defective',\n",
    "    'dark_area_pixels',\n",
    "    'cloud_shadows',\n",
    "    'vegetation',\n",
    "    'not_vegetated',\n",
    "    'water',\n",
    "    'unclassified',\n",
    "    'cloud_medium_probability',\n",
    "    'Cloud_high_probability',\n",
    "    'thin_cirrus',\n",
    "    'snow'\n",
    "]\n",
    "\n",
    "# Permite centrar las etiquetas de los colores en el colorbar\n",
    "classification_labels_ticks = np.linspace(0.5, 11.5, num=12)\n",
    "\n",
    "# Configuración de las etiquetas de la barra de colores\n",
    "img.cbar.set_ticks(classification_labels_ticks)\n",
    "img.cbar.set_ticklabels(classification_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuetro caso particular, estamos interesados en píxeles que presentan probabilidades altas de ser vegetación. Por lo tanto en nuestra máscara de nubes indicamos que deseamos dejar esto píxeles (`qa_clean_mask(dataset,platform='SENTINEL_2',cover_types=['vegetation'])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generación máscara para valores inválidos\n",
    "mask_nan = ~np.isnan(dataset)\n",
    "mask_inf = ~np.isinf(dataset)\n",
    "\n",
    "# Generación máscara de nubes\n",
    "clean_mask = qa_clean_mask(dataset,platform='SENTINEL_2',cover_types=['vegetation'])\n",
    "\n",
    "# Aplicación de la máscara de nubes\n",
    "clean_dataset = dataset.where(clean_mask & mask_nan & mask_inf)\n",
    "clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = clean_dataset[[\"red\",\"green\",\"blue\"]].to_array(dim='color')\n",
    "rgb = rgb.transpose(*(rgb.dims[1:]+rgb.dims[:1]))  # make 'color' the last dimension\n",
    "img = rgb.plot.imshow(col='time',col_wrap=4,add_colorbar=False,vmin=0,vmax=1500)\n",
    "\n",
    "for axes in img.axes.flat:\n",
    "    df_polygon.boundary.plot(ax=axes,markersize=20,color='blue',marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicación del compuesto de medianas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_composite = clean_dataset.median('time', skipna=True)\n",
    "median_composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = median_composite[[\"red\",\"green\",\"blue\"]].to_array(dim='color')\n",
    "rgb = rgb.transpose(*(rgb.dims[1:]+rgb.dims[:1]))  # make 'color' the last dimension\n",
    "img = rgb.plot.imshow(add_colorbar=False,vmin=0,vmax=1500,figsize=(10,10))\n",
    "\n",
    "df_polygon.boundary.plot(ax=img.axes,markersize=20,color='blue',marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Guardar resultados de análisis en fortamto netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar resultados de análisis en un archivo .nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_composite.to_netcdf('mediana.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guardar resultados de análisis en formato geotiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones requeridas para guardar información en geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Las funciones mostradas a continuación fueron tomadas de \n",
    "https://github.com/ceos-seo/data_cube_notebooks\n",
    "\"\"\"\n",
    "\n",
    "def _get_transform_from_xr(data, x_coord='longitude', y_coord='latitude'):\n",
    "    \"\"\"Create a geotransform from an xarray.Dataset or xarray.DataArray.\n",
    "    \"\"\"\n",
    "\n",
    "    from rasterio.transform import from_bounds\n",
    "    geotransform = from_bounds(data[x_coord][0], data[y_coord][-1],\n",
    "                               data[x_coord][-1], data[y_coord][0],\n",
    "                               len(data[x_coord]), len(data[y_coord]))\n",
    "    return geotransform\n",
    "\n",
    "def write_geotiff_from_xr(tif_path, data, bands=None, no_data=-9999, crs=\"EPSG:4326\",\n",
    "                          x_coord='longitude', y_coord='latitude'):\n",
    "    \"\"\"\n",
    "    NOTE: Instead of this function, please use `import_export.export_xarray_to_geotiff()`.\n",
    "    Export a GeoTIFF from an `xarray.Dataset`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    tif_path: string\n",
    "        The path to write the GeoTIFF file to. You should include the file extension.\n",
    "    data: xarray.Dataset or xarray.DataArray\n",
    "    bands: list of string\n",
    "        The bands to write - in the order they should be written.\n",
    "        Ignored if `data` is an `xarray.DataArray`.\n",
    "    no_data: int\n",
    "        The nodata value.\n",
    "    crs: string\n",
    "        The CRS of the output.\n",
    "    x_coord, y_coord: string\n",
    "        The string names of the x and y dimensions.\n",
    "    \"\"\"\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        height, width = data.sizes[y_coord], data.sizes[x_coord]\n",
    "        count, dtype = 1, data.dtype\n",
    "    else:\n",
    "        if bands is None:\n",
    "            bands = list(data.data_vars.keys())\n",
    "        else:\n",
    "            assrt_msg_begin = \"The `data` parameter is an `xarray.Dataset`. \"\n",
    "            assert isinstance(bands, list), assrt_msg_begin + \"Bands must be a list of strings.\"\n",
    "            assert len(bands) > 0 and isinstance(bands[0], str), assrt_msg_begin + \"You must supply at least one band.\"\n",
    "        height, width = data.dims[y_coord], data.dims[x_coord]\n",
    "        count, dtype = len(bands), data[bands[0]].dtype\n",
    "    with rasterio.open(\n",
    "            tif_path,\n",
    "            'w',\n",
    "            driver='GTiff',\n",
    "            height=height,\n",
    "            width=width,\n",
    "            count=count,\n",
    "            dtype=dtype,\n",
    "            crs=crs,\n",
    "            transform=_get_transform_from_xr(data, x_coord=x_coord, y_coord=y_coord),\n",
    "            nodata=no_data) as dst:\n",
    "        if isinstance(data, xr.DataArray):\n",
    "            dst.write(data.values, 1)\n",
    "        else:\n",
    "            for index, band in enumerate(bands):\n",
    "                dst.write(data[band].values, index + 1)\n",
    "    dst.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar resultados de análisis en un archivo .tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_geotiff_from_xr(\n",
    "    tif_path='mediana.tif', \n",
    "    data=median_composite, \n",
    "    # El orden de las bandas es importante al momento de ser visualizada la imágen en una herramienta GIS\n",
    "    bands=['red','green','blue',], \n",
    "    no_data=-9999, \n",
    "    crs=\"EPSG:4326\",\n",
    "    x_coord='longitude',\n",
    "    y_coord='latitude'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
